{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "normal-nursing",
   "metadata": {},
   "source": [
    "## *CoastSat.PlanetScope*: Example at Narrabeen-Collaroy, Australia\n",
    "\n",
    "This notebook shows how to use this toolbox to map shorelines on PlanetScope imagery and extract time-series.\n",
    "\n",
    "The workdlow is divided in three main steps:\n",
    "1. Image pre-processing (cropping, merging, covnversion to TOA, co-registration)\n",
    "2. Shoreline mapping\n",
    "3. Intersection with cross-shore transects and tidal correction\n",
    "\n",
    "The approximate times on a standard laptop for ~1000 images over an area of 10 sqkm are:\n",
    "- 20 min for image pre-processing\n",
    "- 2.5 hrs for co-registration\n",
    "- 35 min to merge the images\n",
    "- 1 hr for image classification\n",
    "- 1 hr for shoreline extraction\n",
    "\n",
    "This notebook shows a short example with 20 images.\n",
    "\n",
    "### Initial settings\n",
    "\n",
    "Refer to the **Installation** section of the README for instructions on how to install the Python packages necessary to run the software.  If that step has been completed correctly, the following packages should be imported without any problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-swedish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load coastsat modules\n",
    "import os\n",
    "from coastsat_ps.data_import import initialise_settings\n",
    "from coastsat_ps.extract_shoreline import extract_shorelines, compute_intersection\n",
    "from coastsat_ps.interactive import filter_shorelines                    \n",
    "from coastsat_ps.preprocess import (data_extract, pre_process, select_ref_image, \n",
    "                                    add_ref_features)\n",
    "from coastsat_ps.postprocess import tidal_correction, ts_plot_single"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-madagascar",
   "metadata": {},
   "source": [
    "### User inputs and general settings\n",
    "\n",
    "The README file also explains how to create the 3 input files that are necessary to run the toolbox at a site:\n",
    "-\ta polygon with the area of interest [as .KML]\n",
    "-\ta transects file [as .GEOJSON]\n",
    "-\ttide data with time in UTC and water level in m AMSL [as .CSV]\n",
    "\n",
    "These should be saved in the */user_inputs* subfolder, example files for Narrabeen are provided. \n",
    "\n",
    "Once these files are ready, users can set up their settings in the dictionary below. Also provide the full path to the downloaded imagery.\n",
    "\n",
    "⚠️ Note that the ‘GDAL_location’ field may vary depending on the OS system, for Windows and Anaconda it should be something like `r'C:\\ProgramData\\Anaconda3\\envs\\coastsat_ps\\Library\\bin'`\n",
    "\n",
    "Some more advanced settings are hard-coded in the  `data_import.py ` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-grace",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \n",
    "    ### General Settings ###\n",
    "    # Site name (for output folder and files) \n",
    "    'site_name': 'NARRA',\n",
    "    # Maximum image cloud cover percentage threshold\n",
    "    'cloud_threshold': 10, # Default 10\n",
    "    # Minimum image AOI cover percentage threshold\n",
    "    'extent_thresh': 80, # Default 80\n",
    "    # Desired output shoreline epsg\n",
    "    'output_epsg': '28356',\n",
    "    \n",
    "    \n",
    "    ### Reference files (in \"...CoastSat.PlanetScope/user_inputs/\") ###\n",
    "    # Area of interest file (save as .kml file from geojson.io website)\n",
    "    'aoi_kml': 'NARRA_polygon.kml',\n",
    "    # Transects in geojson file (ensure same epsg as output_epsg)\n",
    "    'transects': 'NARRA_transects.geojson', # False\n",
    "        # If False boolean given, popup window will allow for manual drawing of transects\n",
    "    # Tide csv file in MSL and UTC \n",
    "    'tide_data': 'NARRA_tides.csv',\n",
    "    # Local folder planet imagery downloads location (provide full folder path)\n",
    "    'downloads_folder': os.path.join(os.getcwd(),'Narrabeen_images_PSScene4Band','files'),\n",
    "\n",
    "\n",
    "    ### Processing settings ###\n",
    "    # Machine learning classifier filename (in \"...CoastSat.PlanetScope/classifier/models\")\n",
    "        # A new classifier may be re-trained after step 1.3. Refer \"...CoastSat.PlanetScope/classifier/train_new_classifier.py\" for instructions. \n",
    "    'classifier': 'NN_4classes_PS_NARRA.pkl',\n",
    "    # Image co-registration choice ['Coreg Off', 'Local Coreg', 'Global Coreg']\n",
    "    'im_coreg': 'Coreg Off', # refer https://pypi.org/project/arosics/ for details on Local vs Global coreg. Local recommended but slower. \n",
    "\n",
    "\n",
    "    ### Advanced settings ###\n",
    "    # Buffer size around masked cloud pixels [in metres]\n",
    "    'cloud_buffer': 9, # Default 9 (3 pixels)  \n",
    "    # Max distance from reference shoreline for valid shoreline [in metres]\n",
    "    'max_dist_ref': 75, # Default 75\n",
    "    # Minimum area (m^2) for an object to be labelled as a beach\n",
    "    'min_beach_area': 150*150, # Default 22500\n",
    "    # Minimum length for identified contour line to be saved as a shoreline [in metres]\n",
    "    'min_length_sl': 500, # Default 500 \n",
    "    # GDAL location setting (Update path to match GDAL path. Update 'coastsat_ps' to chosen environment name. Example provided is for mac)\n",
    "    'GDAL_location': r'C:\\ProgramData\\Anaconda3\\envs\\coastsat_ps\\Library\\bin',\n",
    "    # for Windows\n",
    "    # 'GDAL_location': r'C:\\ProgramData\\Anaconda3\\envs\\coastsat_ps\\Library\\bin',\n",
    "    \n",
    "    #### Additional advanced Settings can be found in \"...CoastSat.PlanetScope/coastsat_ps/data_import.py\"\n",
    "    \n",
    "    }\n",
    "\n",
    "\n",
    "# Import data and updade settings based on user input\n",
    "outputs = initialise_settings(settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-rugby",
   "metadata": {},
   "source": [
    "### 1. Image Preprocessing\n",
    "\n",
    "There are 3 preprocessing steps:\n",
    "- Conversion to Top of Atmosphere (TOA) and image masking (clouds and non usable pixels)\n",
    "- Co-registration (if turned on)\n",
    "- Scene merging\n",
    "\n",
    "#### 1.1. Conversion to TOA and image masking\n",
    "\n",
    "This part converts Digital Numbers to TOA (Top-of-Atmosphere) and extracts the UDMs (Unusable Data Masks) for each image in the folder. The preprocessed images are saved under */outputs/SITENAME/toa_image_data/raw_data*. For each image there are 3 .tif files, the TOA image, a binary cloud mask and a binary NaN mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-czech",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_extract(settings, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-trader",
   "metadata": {},
   "source": [
    "#### 1.2. Select reference image for co-registration\n",
    "\n",
    "The user can choose one image that will be used to align all the others in the next step, make sure this image is cloud free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-casino",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_ref_image(settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-motorcycle",
   "metadata": {},
   "source": [
    "#### 1.3. Image co-registration and scene merging\n",
    "\n",
    "When preprocessing the images in this step, there is the option to delete the intermediate co-registration files to save memory.\n",
    "\n",
    "⚠️ Note that a \"Failed to delete GEOS geom\" error message may appear in console during co-registration, but this does not impact the algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-manitoba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del_files_int = True will delete intermediate coregistration files to save space\n",
    "pre_process(settings, outputs, del_files_int = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquired-nursing",
   "metadata": {},
   "source": [
    "### 2. Extract shorelines\n",
    "\n",
    "First the user has to select a cloud-free image and digitise a reference shoreline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-photograph",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "add_ref_features(settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-scoop",
   "metadata": {},
   "source": [
    "Extract shorelines automatically with the current settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-inspection",
   "metadata": {},
   "outputs": [],
   "source": [
    "shoreline_data = extract_shorelines(outputs, settings,                          \n",
    "        # del_index = True will delete water index .tif files once used to save space\n",
    "        del_index = False, \n",
    "        # reclassify = True will reclassify images if they have been classified previously\n",
    "            # useful when running again with a new classifier\n",
    "            # use False to save time on re-runs with the same classifier to save processing time\n",
    "        reclassify = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-frost",
   "metadata": {},
   "source": [
    "The outputs from the shoreline detection are located at */outputs/SITENAME/shoreline outputs* and include: \n",
    "- a .geojson file with all the 2D shorelines as a geospatial layer, ready to be imported in a GIS environment (note that these are not tidally-corrected)\n",
    "- plots with all the satellite images and detected shorelines saved as .jpg in the */Shoreline plots* subfolder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-richardson",
   "metadata": {},
   "source": [
    "### 3. Manual Quality Control of the shorelines\n",
    "\n",
    "In this step the user can manually select or reject the detected shorelines.\n",
    "\n",
    "Note that classifier is in /coastsat_ps/classifier/models and was trained on Narrabeen pixels. However, there is a script to retrain the classifier for another site if necessary, `train_new_classifier.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-greene",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1:\n",
    "    # manual_filter & load_csv = False\n",
    "        # All images pass, creates a csv in the outputs folder\n",
    "            # \"...CoastSat.PlanetScope/outputs/SITE/shoreline outputs/COREG/NmB/Peak Fraction/shoreline_filter.csv\"\n",
    "\n",
    "# Option 2:\n",
    "    # manual_filter = True & load_csv = False    \n",
    "        # popup window to keep or discard images (saves choices as a csv)\n",
    "\n",
    "# Option 3:\n",
    "    # manual_filter = False & load_csv = True\n",
    "        # loads and applies the csv saved from option 1 or 2\n",
    "        # This file can be manually updated if desired with a text editor\n",
    "         \n",
    "shoreline_data = filter_shorelines(settings, manual_filter = True, load_csv = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-adapter",
   "metadata": {},
   "source": [
    "### 4.  Compute intersections between shorelines and transects\n",
    "\n",
    "The time-series of shoreline change are saved at */outputs/SITENAME/shoreline outputs* in a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-irish",
   "metadata": {},
   "outputs": [],
   "source": [
    "sl_csv = compute_intersection(shoreline_data, settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-findings",
   "metadata": {},
   "source": [
    "### 5. TIdal correction\n",
    "\n",
    "The tidally-corrected time-series of shoreline change are also saved at */outputs/SITENAME/shoreline outputs* in a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-engine",
   "metadata": {},
   "outputs": [],
   "source": [
    "tide_settings = {\n",
    "    # select beach slope as a generic value, or list of values corresponding to each transect\n",
    "    # Transect specific beach slope values can be extracted with the CoastSat beach slope tool https://github.com/kvos/CoastSat.slope\n",
    "    'beach_slope': [0.085, 0.075, 0.08, 0.08, 0.1], #0.1\n",
    "    \n",
    "    # Reference elevation contour\n",
    "    'contour': 0.7,\n",
    "    # Tidal correction weighting\n",
    "    'weighting': 1,\n",
    "    # Offset correction (+ve value corrects sl seaward, ie. increases chainage)\n",
    "    'offset': 0,\n",
    "    \n",
    "    # Date filter (minimum)\n",
    "    'date_min':'2016-01-01',\n",
    "    # Date filter (maximum)\n",
    "    'date_max':'2021-01-01' \n",
    "    }\n",
    "\n",
    "sl_csv_tide = tidal_correction(settings, tide_settings, sl_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-square",
   "metadata": {},
   "source": [
    "### 6. Plot the time-series\n",
    "\n",
    "There is a built-in function to generate plots with the shoreline time-series along the transects. The figures are saved as .jpg in the */Timeseries plots* subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-interview",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through each transect and save a plot of the cross-shore shoreline change along that transect\n",
    "for transect in settings['transects_load'].keys():\n",
    "    ts_plot_single(settings, sl_csv_tide, transect,            \n",
    "        # set savgol = True to plot 15 day moving average shoreline position\n",
    "        # Requires > 15 day shorleine timeseries range\n",
    "        savgol = False,\n",
    "        # set x_scale for x-axis labels ['days', 'months', 'years']\n",
    "        x_scale = 'days')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
